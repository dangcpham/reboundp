<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>reboundp.parallel API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>reboundp.parallel</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># author: Dang Pham
# last modified: December 2023

# Python standard library
import time, os, warnings, math, inspect, datetime
from typing import List

# Third-party libraries
from joblib import Parallel, delayed
import joblib.parallel

# Check which extra features are available
global FEATURES
FEATURES = []

try:
    import rebound
    # only rebound &gt;= 4.0.0 has the webserver feature
    if int(rebound.__version__.split(&#34;.&#34;)[0]) &lt; 4: raise ImportError
    import urllib3
    FEATURES.append(&#34;port&#34;)
except ImportError:
    pass

# try:
#     import psutil
#     import flask
#     import numpy as np
#     import bokeh
#     FEATURES.append(&#34;dashboard&#34;)
# except ImportError:
#     pass

# Local imports
from . import port_utils
from . import utils

REB_STATUS = {-3: &#34;paused&#34;, -2: &#34;laststep&#34;, -1: &#34;running&#34;, 0: &#34;finished&#34;,
              1: &#34;generic_error&#34;, 2: &#34;no_particles&#34;, 3: &#34;encounter&#34;,
              4: &#34;escape&#34;, 5: &#34;user&#34;, 6: &#34;sigint&#34;, 7: &#34;collision&#34;,}

def print_progress(n_completed_tasks:int, joblib_n_jobs:int, joblib_t0:float):
    progress = (n_completed_tasks+1) / joblib_n_jobs
    time_elapsed = utils.time_format(time.time() - joblib_t0)
    output = f&#34;\rProgress: [{&#39;■&#39;*int(progress * 50):&lt;50}] {(progress*100):.1f}% [{n_completed_tasks+1}/{joblib_n_jobs} Tasks] [{time_elapsed}]&#34;
    print(output, end=&#34;&#34;, flush=True)

    if n_completed_tasks+1 &gt;= joblib_n_jobs:
        time_started = datetime.datetime.fromtimestamp(joblib_t0).strftime(&#34;on %Y-%m-%d at %H:%M:%S&#34;)
        print(f&#34;\nFinished running {joblib_n_jobs} tasks. Started {time_started}. Walltime: {utils.time_format(time.time() - joblib_t0)}. \n&#34;)

class TimedBatchCompletionCallBack(joblib.parallel.BatchCompletionCallBack):
    &#34;&#34;&#34; Custom callback for `joblib.parallel.Parallel` that prints progress to `stdout`.&#34;&#34;&#34;
     
    def __call__(self, *args, **kwargs):
        if self.parallel.progressbar:
            print_progress(self.parallel.n_completed_tasks,
                           self.parallel.joblib_n_jobs, 
                           self.parallel.joblib_t0)

        return super().__call__(*args, **kwargs)

class ReboundParallel():
    &#34;&#34;&#34; Class structure for running `REBOUND` simulations in parallel.
        Can be used as decorator or as class to construct objects.
        Uses `joblib.Parallel` in the backend to run simulations in parallel.
    &#34;&#34;&#34;
    def __init__(self, simfunc:callable, 
                 cores:int=None, progressbar:bool=False, 
                 port_buffer:int=10, port0:int=None):
        &#34;&#34;&#34; Initialize `REBOUNDParallel` object.

            Parameters
            ----------
            simfunc : callable
                Function to run in parallel.\n

                -------
                Acceptable signatures for `simfunc`:

                1. `simfunc()`                       --- function takes no argument, **cannot** access sims while running
                2. `simfunc(arg1, arg2, ...)`        --- function takes argument(s), **cannot** access sims while running
                3. `simfunc(port)`                   --- function takes no argument, can access sims while running
                4. `simfunc(port, arg1, arg2, ...)`  --- function takes argument(s), can access sims while running
                -------

            cores : int, optional
                Number of cores to use. Must be a positive, non-zero integer. 
                Default is `None`, which will use all but one core.
            progressbar : bool, optional
                Whether to print a progress bar to stdout. Default is `False`.
            port_buffer : int, optional
                A buffer (we reserve more ports than we need) to avoid overusing ports.
                Recommend to use a number higher than 5 to make sure ports are not overused.
                Default is `10`.
            port0 : int, optional
                First port to use. Must be a positive, non-zero integer, between `1024` and `65535`.
                Default is `None`, which will automatically determine and use first available port..
        &#34;&#34;&#34;
        self.features = FEATURES
        self.cpu_count = os.cpu_count()
        self.simfunc = simfunc

        # get properties of simfunc
        self.simfunc_port = self.simfunc_check()

        # if cores is not set, use all but one core (last one to run this)
        if cores is None:
            self.cores = self.cpu_count - 1
        else:
            self.cores = cores

        # port things
        self.port_buffer = port_buffer
        self.server_path = &#34;http://localhost&#34;
        self.progressbar = progressbar

        # if port0 is not set, use first available port
        if port0 is None: 
            self.port0 = port_utils.first_available_port()
        else:
            self.port0 = port0

        self.results = None
        self.validate_init()

    def simfunc_check(self)-&gt;bool:
        &#34;&#34;&#34; Get properties of `simfunc` and check if simfunc is in valid form.
            Returns whether port is an argument.

            Returns:
            --------
            simfunc_port : bool
                Whether port is an argument in `simfunc`
        &#34;&#34;&#34;
        if callable(self.simfunc) == False:
            raise TypeError(f&#34;simfunc must be a function. {type(self.simfunc)} was passed.&#34;)

        if inspect.signature(self.simfunc).parameters.get(&#34;port&#34;) is None:
            simfunc_port = False
        else:
            simfunc_port = True

        if (simfunc_port == True and 
            list(inspect.signature(self.simfunc).parameters).index(&#34;port&#34;) != 0):
            raise TypeError(f&#34;port must be the first argument in {self.simfunc.__name__}&#34;)

        return simfunc_port

    def validate_init(self):
        &#34;&#34;&#34; Validate ReboundParallel object after initialization.
            Raises TypeError if any of the parameters are invalid.
        &#34;&#34;&#34;
        if self.port0 &gt; 65535 or self.port0 &lt; 1024:
            raise TypeError(&#34;port0 must be a positive integer between 1024 and 65535&#34;)

        if type(self.cores) != int or self.cores &lt; 1:
            raise TypeError(&#34;cores must be a non-zero positive integer&#34;)

        if type(self.port_buffer) != int or self.port_buffer &lt; 1:
            raise TypeError(&#34;port_buffer must be a non-zero positive integer&#34;)

        if type(self.progressbar) != bool:
            raise TypeError(&#34;progressbar must be a boolean&#34;)

    def verify_before_run(self):
        &#34;&#34;&#34; Validate ReboundParallel object before parallel running.
            Raises ValueError if any of the parameters are not set.
        &#34;&#34;&#34;
        if self.njobs is None:
            raise ValueError(&#34;njobs must be set before running&#34;)
        if self.ports_array is None:
            raise ValueError(&#34;ports_array must be set before running&#34;)

    def process_jobs(self, jobs):
        &#34;&#34;&#34; Process jobs argument. Check if jobs is an integer or an iterable.
            If integer, return a list of length `njobs`.
            If iterable and 1D, return a (0,1) array (2D).

            Parameters
            ----------
            jobs: int, list, numpy array
                Number of jobs to run, or list of arguments to run in parallel.

            Returns
            -------
            __jobs : int, list or numpy array
                Number of jobs to run, or list of arguments to run in parallel (processed).

        &#34;&#34;&#34;
        # validate jobs type
        if type(jobs) == int and jobs &gt; 0:
            self.njobs = jobs
        elif utils.is_list(jobs):
            self.njobs = len(jobs)
        else:
            raise TypeError(&#34;jobs must be a positive integer, list, or numpy array&#34;)

        # if jobs is an integer, create a list of length njobs
        if type(jobs) == int:
            __jobs = jobs

        # if jobs is a list
        elif type(jobs) == list:
            # if jobs is a 1D list create a (0,1) list (2 dimensional)
            if len(utils.dim(jobs)) == 1:
                __jobs = [[jobs[i]] for i in range(self.njobs)]
            # if jobs is a 2D list or numpy array, return as is
            elif len(utils.dim(jobs)) == 2:
                __jobs = jobs
            else:
                raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array&#34;)

        # if jobs is a numpy array (or something else similar)
        else:
            # if jobs is a 1D list create a (0,1) list (2 dimensional)
            if jobs.ndim == 1:
                __jobs = [[jobs[i]] for i in range(self.njobs)]
            # if jobs is a 2D list or numpy array, return as is
            elif jobs.ndim == 2:
                __jobs = jobs
            else:
                raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array,&#34;)
    
        return __jobs

    def reset_run(self):
        &#34;&#34;&#34; Reset ReboundParallel object&#39;s parameters: njobs, ports_array, results.&#34;&#34;&#34;
        if self.results is not None:
            warnings.warn(&#34;Results reset; be careful when using &lt;reboundp.results&gt;&#34;, ResourceWarning)

        self.njobs = None
        self.ports_array = None
        self.results = None

    def check_port_feature(self):
        &#34;&#34;&#34; Check if port feature is available.
            Raises ImportError if port feature is not available.
        &#34;&#34;&#34;
        if &#34;port&#34; not in self.features:
            raise ImportError(&#34;Please install reboundp with pip install reboundp[port] to use this function.&#34;)
    
    def current_open_ports(self)-&gt;List[int]:
        &#34;&#34;&#34; Get list of ports currently in use by `REBOUND` servers.

            Parameters
            ----------
            None

            Returns
            -------
            open_ports : list
                List of ports currently in use by `REBOUND` servers
        &#34;&#34;&#34;
        open_ports = port_utils.get_rebound_ports(min(self.ports_array), 
                                    max(self.ports_array) + 2,
                                    server_path=self.server_path)
        return open_ports

    def send_space(self, port:int):
        &#34;&#34;&#34; Send spacebar command to `REBOUND` server at port to pause simulation.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to send spacebar command to
        &#34;&#34;&#34;
        self.check_port_feature()
        urllib3.request(method = &#34;GET&#34;,
                        url = f&#34;{self.server_path}:{port}/keyboard/32&#34;,
                        retries = False)

    def send_q(self, port:int):
        &#34;&#34;&#34; Send q command to `REBOUND` server at port to end simulation.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to send q command to
        &#34;&#34;&#34;
        self.check_port_feature()
        urllib3.request(method = &#34;GET&#34;,
                        url = f&#34;{self.server_path}:{port}/keyboard/81&#34;,
                        retries = 1)

    def fetch_sim(self, port:int):
        &#34;&#34;&#34; Fetch simulation object from `REBOUND` server at port.\n
            Under the hood, this function uses `urllib3` to send a GET request to the server.
            Then, it loads the retrieved bytes data into a `rebound.Simulation` object.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to fetch simulation from

            Returns
            -------
            sim : rebound.Simulation
                Simulation object from `REBOUND` server at port
        &#34;&#34;&#34;
        self.check_port_feature()
        reb_request = urllib3.request(&#34;GET&#34;, 
                                    f&#34;{self.server_path}:{port}/simulation&#34;)
        sim = rebound.Simulation(reb_request.data)

        return sim

    def pause_sim(self, port:int):
        &#34;&#34;&#34; Pause simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to pause simulation
        &#34;&#34;&#34;
        if REB_STATUS[self.fetch_sim(port)._status] == &#34;running&#34;:
            self.send_space(port)

    def pause_all(self):
        &#34;&#34;&#34; Pause all simulations available on ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.pause_sim(port)

    def start_sim(self, port:int):
        &#34;&#34;&#34; Unpause simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to unpause simulation
        &#34;&#34;&#34;
        if REB_STATUS[self.fetch_sim(port)._status] == &#34;paused&#34;:
            self.send_space(port)

    def start_all(self):
        &#34;&#34;&#34; Unpause all simulations available on ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.start_sim(port)

    def end_sim(self, port:int):
        &#34;&#34;&#34; End simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to end simulation
        &#34;&#34;&#34;
        self.check_port_feature()
        try:
            self.send_q(port)
        except urllib3.exceptions.MaxRetryError:
            pass

    def end_all_current_sims(self):
        &#34;&#34;&#34; End all simulations available on REBOUND ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.end_sim(port)

    def end_all(self, sleep_timer:float=0.05, batch_buffer:int=10):
        &#34;&#34;&#34; End all simulations currently available on ports, repeat until all sims have ended.
            Will wait for `batch_buffer` times the number of batches it took to run the simulations.
            (This is a heuristic to ensure that all sims end.)

            Parameters
            ----------
            sleep_timer : float, optional
                Time to sleep between checking if all sims have ended. Default is 0.01.
            batch_buffer : int, optional
                Batch buffer to ensure that all sims are ended. Default is 10.
        &#34;&#34;&#34;
        nbatches = int(math.ceil(self.njobs / self.cores)) * batch_buffer
        warnings.warn(&#34;Ending all tasks ...&#34;, UserWarning)
        for _ in range(nbatches):
            self.end_all_current_sims()
            time.sleep(sleep_timer)

    def run(self, jobs, cores:int=None, progressbar:bool=None, 
            *joblib_args, **joblib_kwargs)-&gt;List:
        &#34;&#34;&#34; Run jobs in parallel.

            Parameters
            ----------
            jobs : iterable (int, list or numpy array. If int, must be positive. If iterable, 1 or 2 dimensional.)
                Number of time to run `simfunc`, or list of arguments to distribute to run in parallel.\n
                Will run `simfunc(port, *args)` for each job. 
                Port is automatically assigned by `ReboundParallel`.\n
                `run(jobs)` dispatches jobs as follows:

                1. If `jobs` is an integer, will run `simfunc` for `jobs` number of time.
                1. If `jobs` is a 1D list or numpy array, will run `simfunc(port, *jobs[i])` for each job.
            cores : int, optional
                Number of cores to use. Must be a positive, non-zero integer. 
                Default is None, which will use all but one core.
            progressbar : bool, optional
                Whether to print a progress bar to stdout. 
                If not set, will use value from initialization (default False).
            *joblib_args : optional
                Additional arguments to pass to joblib.Parallel
            **joblib_kwargs : optional
                Additional keyword arguments to pass to joblib.Parallel

            Returns
            -------
            results : List
                List of results from running jobs in parallel        
        &#34;&#34;&#34;
        # reset before running
        self.reset_run()

        # validate and process jobs argument
        jobs = self.process_jobs(jobs)

        # handle cores and progressbar arguments
        if cores is not None: self.cores = cores
        if progressbar is not None: self.progressbar = progressbar
        if self.progressbar and self.cores == 1: 
            print(&#34;Running in serial mode.&#34;)
        elif self.progressbar and self.cores &gt; 1:
            print(f&#34;Running in parallel mode with {self.cores} cores.&#34;)
        

        # assign ports to jobs
        job_list = list(range(0, self.njobs))
        port1 = self.port0 + 1
        core_buffer = self.cores * self.port_buffer
        self.ports_array = [(port1 + (port%core_buffer)) for port in job_list]

        # validate before running
        self.validate_init()
        self.verify_before_run()

        # track progress
        __n_completed_tasks = 0
        __t0 = time.time()
        if self.progressbar: print_progress(__n_completed_tasks, self.njobs, __t0)

        if self.cores == 1:
            # output list
            results = []

            for i in range(self.njobs):
                if type(jobs) == int:
                    results.append(self.simfunc())
                else:
                    if self.simfunc_port:
                        results.append(self.simfunc(self.ports_array[i],
                                                    *jobs[i]))
                    else:
                        results.append(self.simfunc(*jobs[i]))
                
                if __n_completed_tasks+1 &lt; self.njobs:
                    print_progress(__n_completed_tasks+1, self.njobs, __t0)
                __n_completed_tasks += 1
        else:
            # run jobs in parallel
            joblib.parallel.BatchCompletionCallBack = TimedBatchCompletionCallBack
            with Parallel(n_jobs=self.cores, 
                          *joblib_args, **joblib_kwargs) as parallel:
                # track progress
                parallel.joblib_n_jobs = self.njobs
                parallel.progressbar = self.progressbar
                parallel.joblib_t0 = __t0

                if type(jobs) == int:
                    results = parallel(delayed(self.simfunc)() 
                                       for i in range(self.njobs))
                else:
                    if self.simfunc_port:
                        results = parallel(delayed(self.simfunc)(
                            self.ports_array[i], *jobs[i]) 
                            for i in range(self.njobs))
                    else:
                        results = parallel(delayed(self.simfunc)(*jobs[i])
                                        for i in range(self.njobs))

        self.results = results
        return results</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="reboundp.parallel.print_progress"><code class="name flex">
<span>def <span class="ident">print_progress</span></span>(<span>n_completed_tasks: int, joblib_n_jobs: int, joblib_t0: float)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def print_progress(n_completed_tasks:int, joblib_n_jobs:int, joblib_t0:float):
    progress = (n_completed_tasks+1) / joblib_n_jobs
    time_elapsed = utils.time_format(time.time() - joblib_t0)
    output = f&#34;\rProgress: [{&#39;■&#39;*int(progress * 50):&lt;50}] {(progress*100):.1f}% [{n_completed_tasks+1}/{joblib_n_jobs} Tasks] [{time_elapsed}]&#34;
    print(output, end=&#34;&#34;, flush=True)

    if n_completed_tasks+1 &gt;= joblib_n_jobs:
        time_started = datetime.datetime.fromtimestamp(joblib_t0).strftime(&#34;on %Y-%m-%d at %H:%M:%S&#34;)
        print(f&#34;\nFinished running {joblib_n_jobs} tasks. Started {time_started}. Walltime: {utils.time_format(time.time() - joblib_t0)}. \n&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="reboundp.parallel.ReboundParallel"><code class="flex name class">
<span>class <span class="ident">ReboundParallel</span></span>
<span>(</span><span>simfunc: <built-in function callable>, cores: int = None, progressbar: bool = False, port_buffer: int = 10, port0: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class structure for running <code>REBOUND</code> simulations in parallel.
Can be used as decorator or as class to construct objects.
Uses <code>joblib.Parallel</code> in the backend to run simulations in parallel.</p>
<p>Initialize <code>REBOUNDParallel</code> object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>simfunc</code></strong> :&ensp;<code>callable</code></dt>
<dd>
<p>Function to run in parallel.</p>
<hr>
<p>Acceptable signatures for <code>simfunc</code>:</p>
<ol>
<li><code>simfunc()</code>
&mdash; function takes no argument, <strong>cannot</strong> access sims while running</li>
<li><code>simfunc(arg1, arg2, &hellip;)</code>
&mdash; function takes argument(s), <strong>cannot</strong> access sims while running</li>
<li><code>simfunc(port)</code>
&mdash; function takes no argument, can access sims while running</li>
<li><code>simfunc(port, arg1, arg2, &hellip;)</code>
&mdash; function takes argument(s), can access sims while running</li>
</ol>
<hr>
</dd>
<dt><strong><code>cores</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of cores to use. Must be a positive, non-zero integer.
Default is <code>None</code>, which will use all but one core.</dd>
<dt><strong><code>progressbar</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print a progress bar to stdout. Default is <code>False</code>.</dd>
<dt><strong><code>port_buffer</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>A buffer (we reserve more ports than we need) to avoid overusing ports.
Recommend to use a number higher than 5 to make sure ports are not overused.
Default is <code>10</code>.</dd>
<dt><strong><code>port0</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>First port to use. Must be a positive, non-zero integer, between <code>1024</code> and <code>65535</code>.
Default is <code>None</code>, which will automatically determine and use first available port..</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReboundParallel():
    &#34;&#34;&#34; Class structure for running `REBOUND` simulations in parallel.
        Can be used as decorator or as class to construct objects.
        Uses `joblib.Parallel` in the backend to run simulations in parallel.
    &#34;&#34;&#34;
    def __init__(self, simfunc:callable, 
                 cores:int=None, progressbar:bool=False, 
                 port_buffer:int=10, port0:int=None):
        &#34;&#34;&#34; Initialize `REBOUNDParallel` object.

            Parameters
            ----------
            simfunc : callable
                Function to run in parallel.\n

                -------
                Acceptable signatures for `simfunc`:

                1. `simfunc()`                       --- function takes no argument, **cannot** access sims while running
                2. `simfunc(arg1, arg2, ...)`        --- function takes argument(s), **cannot** access sims while running
                3. `simfunc(port)`                   --- function takes no argument, can access sims while running
                4. `simfunc(port, arg1, arg2, ...)`  --- function takes argument(s), can access sims while running
                -------

            cores : int, optional
                Number of cores to use. Must be a positive, non-zero integer. 
                Default is `None`, which will use all but one core.
            progressbar : bool, optional
                Whether to print a progress bar to stdout. Default is `False`.
            port_buffer : int, optional
                A buffer (we reserve more ports than we need) to avoid overusing ports.
                Recommend to use a number higher than 5 to make sure ports are not overused.
                Default is `10`.
            port0 : int, optional
                First port to use. Must be a positive, non-zero integer, between `1024` and `65535`.
                Default is `None`, which will automatically determine and use first available port..
        &#34;&#34;&#34;
        self.features = FEATURES
        self.cpu_count = os.cpu_count()
        self.simfunc = simfunc

        # get properties of simfunc
        self.simfunc_port = self.simfunc_check()

        # if cores is not set, use all but one core (last one to run this)
        if cores is None:
            self.cores = self.cpu_count - 1
        else:
            self.cores = cores

        # port things
        self.port_buffer = port_buffer
        self.server_path = &#34;http://localhost&#34;
        self.progressbar = progressbar

        # if port0 is not set, use first available port
        if port0 is None: 
            self.port0 = port_utils.first_available_port()
        else:
            self.port0 = port0

        self.results = None
        self.validate_init()

    def simfunc_check(self)-&gt;bool:
        &#34;&#34;&#34; Get properties of `simfunc` and check if simfunc is in valid form.
            Returns whether port is an argument.

            Returns:
            --------
            simfunc_port : bool
                Whether port is an argument in `simfunc`
        &#34;&#34;&#34;
        if callable(self.simfunc) == False:
            raise TypeError(f&#34;simfunc must be a function. {type(self.simfunc)} was passed.&#34;)

        if inspect.signature(self.simfunc).parameters.get(&#34;port&#34;) is None:
            simfunc_port = False
        else:
            simfunc_port = True

        if (simfunc_port == True and 
            list(inspect.signature(self.simfunc).parameters).index(&#34;port&#34;) != 0):
            raise TypeError(f&#34;port must be the first argument in {self.simfunc.__name__}&#34;)

        return simfunc_port

    def validate_init(self):
        &#34;&#34;&#34; Validate ReboundParallel object after initialization.
            Raises TypeError if any of the parameters are invalid.
        &#34;&#34;&#34;
        if self.port0 &gt; 65535 or self.port0 &lt; 1024:
            raise TypeError(&#34;port0 must be a positive integer between 1024 and 65535&#34;)

        if type(self.cores) != int or self.cores &lt; 1:
            raise TypeError(&#34;cores must be a non-zero positive integer&#34;)

        if type(self.port_buffer) != int or self.port_buffer &lt; 1:
            raise TypeError(&#34;port_buffer must be a non-zero positive integer&#34;)

        if type(self.progressbar) != bool:
            raise TypeError(&#34;progressbar must be a boolean&#34;)

    def verify_before_run(self):
        &#34;&#34;&#34; Validate ReboundParallel object before parallel running.
            Raises ValueError if any of the parameters are not set.
        &#34;&#34;&#34;
        if self.njobs is None:
            raise ValueError(&#34;njobs must be set before running&#34;)
        if self.ports_array is None:
            raise ValueError(&#34;ports_array must be set before running&#34;)

    def process_jobs(self, jobs):
        &#34;&#34;&#34; Process jobs argument. Check if jobs is an integer or an iterable.
            If integer, return a list of length `njobs`.
            If iterable and 1D, return a (0,1) array (2D).

            Parameters
            ----------
            jobs: int, list, numpy array
                Number of jobs to run, or list of arguments to run in parallel.

            Returns
            -------
            __jobs : int, list or numpy array
                Number of jobs to run, or list of arguments to run in parallel (processed).

        &#34;&#34;&#34;
        # validate jobs type
        if type(jobs) == int and jobs &gt; 0:
            self.njobs = jobs
        elif utils.is_list(jobs):
            self.njobs = len(jobs)
        else:
            raise TypeError(&#34;jobs must be a positive integer, list, or numpy array&#34;)

        # if jobs is an integer, create a list of length njobs
        if type(jobs) == int:
            __jobs = jobs

        # if jobs is a list
        elif type(jobs) == list:
            # if jobs is a 1D list create a (0,1) list (2 dimensional)
            if len(utils.dim(jobs)) == 1:
                __jobs = [[jobs[i]] for i in range(self.njobs)]
            # if jobs is a 2D list or numpy array, return as is
            elif len(utils.dim(jobs)) == 2:
                __jobs = jobs
            else:
                raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array&#34;)

        # if jobs is a numpy array (or something else similar)
        else:
            # if jobs is a 1D list create a (0,1) list (2 dimensional)
            if jobs.ndim == 1:
                __jobs = [[jobs[i]] for i in range(self.njobs)]
            # if jobs is a 2D list or numpy array, return as is
            elif jobs.ndim == 2:
                __jobs = jobs
            else:
                raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array,&#34;)
    
        return __jobs

    def reset_run(self):
        &#34;&#34;&#34; Reset ReboundParallel object&#39;s parameters: njobs, ports_array, results.&#34;&#34;&#34;
        if self.results is not None:
            warnings.warn(&#34;Results reset; be careful when using &lt;reboundp.results&gt;&#34;, ResourceWarning)

        self.njobs = None
        self.ports_array = None
        self.results = None

    def check_port_feature(self):
        &#34;&#34;&#34; Check if port feature is available.
            Raises ImportError if port feature is not available.
        &#34;&#34;&#34;
        if &#34;port&#34; not in self.features:
            raise ImportError(&#34;Please install reboundp with pip install reboundp[port] to use this function.&#34;)
    
    def current_open_ports(self)-&gt;List[int]:
        &#34;&#34;&#34; Get list of ports currently in use by `REBOUND` servers.

            Parameters
            ----------
            None

            Returns
            -------
            open_ports : list
                List of ports currently in use by `REBOUND` servers
        &#34;&#34;&#34;
        open_ports = port_utils.get_rebound_ports(min(self.ports_array), 
                                    max(self.ports_array) + 2,
                                    server_path=self.server_path)
        return open_ports

    def send_space(self, port:int):
        &#34;&#34;&#34; Send spacebar command to `REBOUND` server at port to pause simulation.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to send spacebar command to
        &#34;&#34;&#34;
        self.check_port_feature()
        urllib3.request(method = &#34;GET&#34;,
                        url = f&#34;{self.server_path}:{port}/keyboard/32&#34;,
                        retries = False)

    def send_q(self, port:int):
        &#34;&#34;&#34; Send q command to `REBOUND` server at port to end simulation.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to send q command to
        &#34;&#34;&#34;
        self.check_port_feature()
        urllib3.request(method = &#34;GET&#34;,
                        url = f&#34;{self.server_path}:{port}/keyboard/81&#34;,
                        retries = 1)

    def fetch_sim(self, port:int):
        &#34;&#34;&#34; Fetch simulation object from `REBOUND` server at port.\n
            Under the hood, this function uses `urllib3` to send a GET request to the server.
            Then, it loads the retrieved bytes data into a `rebound.Simulation` object.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to fetch simulation from

            Returns
            -------
            sim : rebound.Simulation
                Simulation object from `REBOUND` server at port
        &#34;&#34;&#34;
        self.check_port_feature()
        reb_request = urllib3.request(&#34;GET&#34;, 
                                    f&#34;{self.server_path}:{port}/simulation&#34;)
        sim = rebound.Simulation(reb_request.data)

        return sim

    def pause_sim(self, port:int):
        &#34;&#34;&#34; Pause simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to pause simulation
        &#34;&#34;&#34;
        if REB_STATUS[self.fetch_sim(port)._status] == &#34;running&#34;:
            self.send_space(port)

    def pause_all(self):
        &#34;&#34;&#34; Pause all simulations available on ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.pause_sim(port)

    def start_sim(self, port:int):
        &#34;&#34;&#34; Unpause simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to unpause simulation
        &#34;&#34;&#34;
        if REB_STATUS[self.fetch_sim(port)._status] == &#34;paused&#34;:
            self.send_space(port)

    def start_all(self):
        &#34;&#34;&#34; Unpause all simulations available on ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.start_sim(port)

    def end_sim(self, port:int):
        &#34;&#34;&#34; End simulation at port.

            Parameters
            ----------
            port : int
                Port of `REBOUND` server to end simulation
        &#34;&#34;&#34;
        self.check_port_feature()
        try:
            self.send_q(port)
        except urllib3.exceptions.MaxRetryError:
            pass

    def end_all_current_sims(self):
        &#34;&#34;&#34; End all simulations available on REBOUND ports.&#34;&#34;&#34;
        for port in self.current_open_ports():
            self.end_sim(port)

    def end_all(self, sleep_timer:float=0.05, batch_buffer:int=10):
        &#34;&#34;&#34; End all simulations currently available on ports, repeat until all sims have ended.
            Will wait for `batch_buffer` times the number of batches it took to run the simulations.
            (This is a heuristic to ensure that all sims end.)

            Parameters
            ----------
            sleep_timer : float, optional
                Time to sleep between checking if all sims have ended. Default is 0.01.
            batch_buffer : int, optional
                Batch buffer to ensure that all sims are ended. Default is 10.
        &#34;&#34;&#34;
        nbatches = int(math.ceil(self.njobs / self.cores)) * batch_buffer
        warnings.warn(&#34;Ending all tasks ...&#34;, UserWarning)
        for _ in range(nbatches):
            self.end_all_current_sims()
            time.sleep(sleep_timer)

    def run(self, jobs, cores:int=None, progressbar:bool=None, 
            *joblib_args, **joblib_kwargs)-&gt;List:
        &#34;&#34;&#34; Run jobs in parallel.

            Parameters
            ----------
            jobs : iterable (int, list or numpy array. If int, must be positive. If iterable, 1 or 2 dimensional.)
                Number of time to run `simfunc`, or list of arguments to distribute to run in parallel.\n
                Will run `simfunc(port, *args)` for each job. 
                Port is automatically assigned by `ReboundParallel`.\n
                `run(jobs)` dispatches jobs as follows:

                1. If `jobs` is an integer, will run `simfunc` for `jobs` number of time.
                1. If `jobs` is a 1D list or numpy array, will run `simfunc(port, *jobs[i])` for each job.
            cores : int, optional
                Number of cores to use. Must be a positive, non-zero integer. 
                Default is None, which will use all but one core.
            progressbar : bool, optional
                Whether to print a progress bar to stdout. 
                If not set, will use value from initialization (default False).
            *joblib_args : optional
                Additional arguments to pass to joblib.Parallel
            **joblib_kwargs : optional
                Additional keyword arguments to pass to joblib.Parallel

            Returns
            -------
            results : List
                List of results from running jobs in parallel        
        &#34;&#34;&#34;
        # reset before running
        self.reset_run()

        # validate and process jobs argument
        jobs = self.process_jobs(jobs)

        # handle cores and progressbar arguments
        if cores is not None: self.cores = cores
        if progressbar is not None: self.progressbar = progressbar
        if self.progressbar and self.cores == 1: 
            print(&#34;Running in serial mode.&#34;)
        elif self.progressbar and self.cores &gt; 1:
            print(f&#34;Running in parallel mode with {self.cores} cores.&#34;)
        

        # assign ports to jobs
        job_list = list(range(0, self.njobs))
        port1 = self.port0 + 1
        core_buffer = self.cores * self.port_buffer
        self.ports_array = [(port1 + (port%core_buffer)) for port in job_list]

        # validate before running
        self.validate_init()
        self.verify_before_run()

        # track progress
        __n_completed_tasks = 0
        __t0 = time.time()
        if self.progressbar: print_progress(__n_completed_tasks, self.njobs, __t0)

        if self.cores == 1:
            # output list
            results = []

            for i in range(self.njobs):
                if type(jobs) == int:
                    results.append(self.simfunc())
                else:
                    if self.simfunc_port:
                        results.append(self.simfunc(self.ports_array[i],
                                                    *jobs[i]))
                    else:
                        results.append(self.simfunc(*jobs[i]))
                
                if __n_completed_tasks+1 &lt; self.njobs:
                    print_progress(__n_completed_tasks+1, self.njobs, __t0)
                __n_completed_tasks += 1
        else:
            # run jobs in parallel
            joblib.parallel.BatchCompletionCallBack = TimedBatchCompletionCallBack
            with Parallel(n_jobs=self.cores, 
                          *joblib_args, **joblib_kwargs) as parallel:
                # track progress
                parallel.joblib_n_jobs = self.njobs
                parallel.progressbar = self.progressbar
                parallel.joblib_t0 = __t0

                if type(jobs) == int:
                    results = parallel(delayed(self.simfunc)() 
                                       for i in range(self.njobs))
                else:
                    if self.simfunc_port:
                        results = parallel(delayed(self.simfunc)(
                            self.ports_array[i], *jobs[i]) 
                            for i in range(self.njobs))
                    else:
                        results = parallel(delayed(self.simfunc)(*jobs[i])
                                        for i in range(self.njobs))

        self.results = results
        return results</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="reboundp.parallel.ReboundParallel.check_port_feature"><code class="name flex">
<span>def <span class="ident">check_port_feature</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Check if port feature is available.
Raises ImportError if port feature is not available.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_port_feature(self):
    &#34;&#34;&#34; Check if port feature is available.
        Raises ImportError if port feature is not available.
    &#34;&#34;&#34;
    if &#34;port&#34; not in self.features:
        raise ImportError(&#34;Please install reboundp with pip install reboundp[port] to use this function.&#34;)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.current_open_ports"><code class="name flex">
<span>def <span class="ident">current_open_ports</span></span>(<span>self) ‑> List[int]</span>
</code></dt>
<dd>
<div class="desc"><p>Get list of ports currently in use by <code>REBOUND</code> servers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>open_ports</code></strong> :&ensp;<code>list</code></dt>
<dd>List of ports currently in use by <code>REBOUND</code> servers</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def current_open_ports(self)-&gt;List[int]:
    &#34;&#34;&#34; Get list of ports currently in use by `REBOUND` servers.

        Parameters
        ----------
        None

        Returns
        -------
        open_ports : list
            List of ports currently in use by `REBOUND` servers
    &#34;&#34;&#34;
    open_ports = port_utils.get_rebound_ports(min(self.ports_array), 
                                max(self.ports_array) + 2,
                                server_path=self.server_path)
    return open_ports</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.end_all"><code class="name flex">
<span>def <span class="ident">end_all</span></span>(<span>self, sleep_timer: float = 0.05, batch_buffer: int = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>End all simulations currently available on ports, repeat until all sims have ended.
Will wait for <code>batch_buffer</code> times the number of batches it took to run the simulations.
(This is a heuristic to ensure that all sims end.)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sleep_timer</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Time to sleep between checking if all sims have ended. Default is 0.01.</dd>
<dt><strong><code>batch_buffer</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Batch buffer to ensure that all sims are ended. Default is 10.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_all(self, sleep_timer:float=0.05, batch_buffer:int=10):
    &#34;&#34;&#34; End all simulations currently available on ports, repeat until all sims have ended.
        Will wait for `batch_buffer` times the number of batches it took to run the simulations.
        (This is a heuristic to ensure that all sims end.)

        Parameters
        ----------
        sleep_timer : float, optional
            Time to sleep between checking if all sims have ended. Default is 0.01.
        batch_buffer : int, optional
            Batch buffer to ensure that all sims are ended. Default is 10.
    &#34;&#34;&#34;
    nbatches = int(math.ceil(self.njobs / self.cores)) * batch_buffer
    warnings.warn(&#34;Ending all tasks ...&#34;, UserWarning)
    for _ in range(nbatches):
        self.end_all_current_sims()
        time.sleep(sleep_timer)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.end_all_current_sims"><code class="name flex">
<span>def <span class="ident">end_all_current_sims</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>End all simulations available on REBOUND ports.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_all_current_sims(self):
    &#34;&#34;&#34; End all simulations available on REBOUND ports.&#34;&#34;&#34;
    for port in self.current_open_ports():
        self.end_sim(port)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.end_sim"><code class="name flex">
<span>def <span class="ident">end_sim</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>End simulation at port.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to end simulation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def end_sim(self, port:int):
    &#34;&#34;&#34; End simulation at port.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to end simulation
    &#34;&#34;&#34;
    self.check_port_feature()
    try:
        self.send_q(port)
    except urllib3.exceptions.MaxRetryError:
        pass</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.fetch_sim"><code class="name flex">
<span>def <span class="ident">fetch_sim</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Fetch simulation object from <code>REBOUND</code> server at port.</p>
<p>Under the hood, this function uses <code>urllib3</code> to send a GET request to the server.
Then, it loads the retrieved bytes data into a <code>rebound.Simulation</code> object.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to fetch simulation from</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>sim</code></strong> :&ensp;<code>rebound.Simulation</code></dt>
<dd>Simulation object from <code>REBOUND</code> server at port</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fetch_sim(self, port:int):
    &#34;&#34;&#34; Fetch simulation object from `REBOUND` server at port.\n
        Under the hood, this function uses `urllib3` to send a GET request to the server.
        Then, it loads the retrieved bytes data into a `rebound.Simulation` object.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to fetch simulation from

        Returns
        -------
        sim : rebound.Simulation
            Simulation object from `REBOUND` server at port
    &#34;&#34;&#34;
    self.check_port_feature()
    reb_request = urllib3.request(&#34;GET&#34;, 
                                f&#34;{self.server_path}:{port}/simulation&#34;)
    sim = rebound.Simulation(reb_request.data)

    return sim</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.pause_all"><code class="name flex">
<span>def <span class="ident">pause_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Pause all simulations available on ports.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pause_all(self):
    &#34;&#34;&#34; Pause all simulations available on ports.&#34;&#34;&#34;
    for port in self.current_open_ports():
        self.pause_sim(port)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.pause_sim"><code class="name flex">
<span>def <span class="ident">pause_sim</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Pause simulation at port.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to pause simulation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pause_sim(self, port:int):
    &#34;&#34;&#34; Pause simulation at port.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to pause simulation
    &#34;&#34;&#34;
    if REB_STATUS[self.fetch_sim(port)._status] == &#34;running&#34;:
        self.send_space(port)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.process_jobs"><code class="name flex">
<span>def <span class="ident">process_jobs</span></span>(<span>self, jobs)</span>
</code></dt>
<dd>
<div class="desc"><p>Process jobs argument. Check if jobs is an integer or an iterable.
If integer, return a list of length <code>njobs</code>.
If iterable and 1D, return a (0,1) array (2D).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>jobs</code></strong> :&ensp;<code>int, list, numpy array</code></dt>
<dd>Number of jobs to run, or list of arguments to run in parallel.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>__jobs</code></strong> :&ensp;<code>int, list</code> or <code>numpy array</code></dt>
<dd>Number of jobs to run, or list of arguments to run in parallel (processed).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_jobs(self, jobs):
    &#34;&#34;&#34; Process jobs argument. Check if jobs is an integer or an iterable.
        If integer, return a list of length `njobs`.
        If iterable and 1D, return a (0,1) array (2D).

        Parameters
        ----------
        jobs: int, list, numpy array
            Number of jobs to run, or list of arguments to run in parallel.

        Returns
        -------
        __jobs : int, list or numpy array
            Number of jobs to run, or list of arguments to run in parallel (processed).

    &#34;&#34;&#34;
    # validate jobs type
    if type(jobs) == int and jobs &gt; 0:
        self.njobs = jobs
    elif utils.is_list(jobs):
        self.njobs = len(jobs)
    else:
        raise TypeError(&#34;jobs must be a positive integer, list, or numpy array&#34;)

    # if jobs is an integer, create a list of length njobs
    if type(jobs) == int:
        __jobs = jobs

    # if jobs is a list
    elif type(jobs) == list:
        # if jobs is a 1D list create a (0,1) list (2 dimensional)
        if len(utils.dim(jobs)) == 1:
            __jobs = [[jobs[i]] for i in range(self.njobs)]
        # if jobs is a 2D list or numpy array, return as is
        elif len(utils.dim(jobs)) == 2:
            __jobs = jobs
        else:
            raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array&#34;)

    # if jobs is a numpy array (or something else similar)
    else:
        # if jobs is a 1D list create a (0,1) list (2 dimensional)
        if jobs.ndim == 1:
            __jobs = [[jobs[i]] for i in range(self.njobs)]
        # if jobs is a 2D list or numpy array, return as is
        elif jobs.ndim == 2:
            __jobs = jobs
        else:
            raise ValueError(f&#34;jobs must be a 1 or 2 dimensional array,&#34;)

    return __jobs</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.reset_run"><code class="name flex">
<span>def <span class="ident">reset_run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Reset ReboundParallel object's parameters: njobs, ports_array, results.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_run(self):
    &#34;&#34;&#34; Reset ReboundParallel object&#39;s parameters: njobs, ports_array, results.&#34;&#34;&#34;
    if self.results is not None:
        warnings.warn(&#34;Results reset; be careful when using &lt;reboundp.results&gt;&#34;, ResourceWarning)

    self.njobs = None
    self.ports_array = None
    self.results = None</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, jobs, cores: int = None, progressbar: bool = None, *joblib_args, **joblib_kwargs) ‑> List</span>
</code></dt>
<dd>
<div class="desc"><p>Run jobs in parallel.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>jobs</code></strong> :&ensp;<code>iterable (int, list</code> or <code>numpy array. If int, must be positive. If iterable, 1</code> or <code>2 dimensional.)</code></dt>
<dd>
<p>Number of time to run <code>simfunc</code>, or list of arguments to distribute to run in parallel.</p>
<p>Will run <code>simfunc(port, *args)</code> for each job.
Port is automatically assigned by <code><a title="reboundp.parallel.ReboundParallel" href="#reboundp.parallel.ReboundParallel">ReboundParallel</a></code>.</p>
<p><code>run(jobs)</code> dispatches jobs as follows:</p>
<ol>
<li>If <code>jobs</code> is an integer, will run <code>simfunc</code> for <code>jobs</code> number of time.</li>
<li>If <code>jobs</code> is a 1D list or numpy array, will run <code>simfunc(port, *jobs[i])</code> for each job.</li>
</ol>
</dd>
<dt><strong><code>cores</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of cores to use. Must be a positive, non-zero integer.
Default is None, which will use all but one core.</dd>
<dt><strong><code>progressbar</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to print a progress bar to stdout.
If not set, will use value from initialization (default False).</dd>
<dt><strong><code>*joblib_args</code></strong> :&ensp;<code>optional</code></dt>
<dd>Additional arguments to pass to joblib.Parallel</dd>
<dt><strong><code>**joblib_kwargs</code></strong> :&ensp;<code>optional</code></dt>
<dd>Additional keyword arguments to pass to joblib.Parallel</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>List</code></dt>
<dd>List of results from running jobs in parallel</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self, jobs, cores:int=None, progressbar:bool=None, 
        *joblib_args, **joblib_kwargs)-&gt;List:
    &#34;&#34;&#34; Run jobs in parallel.

        Parameters
        ----------
        jobs : iterable (int, list or numpy array. If int, must be positive. If iterable, 1 or 2 dimensional.)
            Number of time to run `simfunc`, or list of arguments to distribute to run in parallel.\n
            Will run `simfunc(port, *args)` for each job. 
            Port is automatically assigned by `ReboundParallel`.\n
            `run(jobs)` dispatches jobs as follows:

            1. If `jobs` is an integer, will run `simfunc` for `jobs` number of time.
            1. If `jobs` is a 1D list or numpy array, will run `simfunc(port, *jobs[i])` for each job.
        cores : int, optional
            Number of cores to use. Must be a positive, non-zero integer. 
            Default is None, which will use all but one core.
        progressbar : bool, optional
            Whether to print a progress bar to stdout. 
            If not set, will use value from initialization (default False).
        *joblib_args : optional
            Additional arguments to pass to joblib.Parallel
        **joblib_kwargs : optional
            Additional keyword arguments to pass to joblib.Parallel

        Returns
        -------
        results : List
            List of results from running jobs in parallel        
    &#34;&#34;&#34;
    # reset before running
    self.reset_run()

    # validate and process jobs argument
    jobs = self.process_jobs(jobs)

    # handle cores and progressbar arguments
    if cores is not None: self.cores = cores
    if progressbar is not None: self.progressbar = progressbar
    if self.progressbar and self.cores == 1: 
        print(&#34;Running in serial mode.&#34;)
    elif self.progressbar and self.cores &gt; 1:
        print(f&#34;Running in parallel mode with {self.cores} cores.&#34;)
    

    # assign ports to jobs
    job_list = list(range(0, self.njobs))
    port1 = self.port0 + 1
    core_buffer = self.cores * self.port_buffer
    self.ports_array = [(port1 + (port%core_buffer)) for port in job_list]

    # validate before running
    self.validate_init()
    self.verify_before_run()

    # track progress
    __n_completed_tasks = 0
    __t0 = time.time()
    if self.progressbar: print_progress(__n_completed_tasks, self.njobs, __t0)

    if self.cores == 1:
        # output list
        results = []

        for i in range(self.njobs):
            if type(jobs) == int:
                results.append(self.simfunc())
            else:
                if self.simfunc_port:
                    results.append(self.simfunc(self.ports_array[i],
                                                *jobs[i]))
                else:
                    results.append(self.simfunc(*jobs[i]))
            
            if __n_completed_tasks+1 &lt; self.njobs:
                print_progress(__n_completed_tasks+1, self.njobs, __t0)
            __n_completed_tasks += 1
    else:
        # run jobs in parallel
        joblib.parallel.BatchCompletionCallBack = TimedBatchCompletionCallBack
        with Parallel(n_jobs=self.cores, 
                      *joblib_args, **joblib_kwargs) as parallel:
            # track progress
            parallel.joblib_n_jobs = self.njobs
            parallel.progressbar = self.progressbar
            parallel.joblib_t0 = __t0

            if type(jobs) == int:
                results = parallel(delayed(self.simfunc)() 
                                   for i in range(self.njobs))
            else:
                if self.simfunc_port:
                    results = parallel(delayed(self.simfunc)(
                        self.ports_array[i], *jobs[i]) 
                        for i in range(self.njobs))
                else:
                    results = parallel(delayed(self.simfunc)(*jobs[i])
                                    for i in range(self.njobs))

    self.results = results
    return results</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.send_q"><code class="name flex">
<span>def <span class="ident">send_q</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Send q command to <code>REBOUND</code> server at port to end simulation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to send q command to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_q(self, port:int):
    &#34;&#34;&#34; Send q command to `REBOUND` server at port to end simulation.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to send q command to
    &#34;&#34;&#34;
    self.check_port_feature()
    urllib3.request(method = &#34;GET&#34;,
                    url = f&#34;{self.server_path}:{port}/keyboard/81&#34;,
                    retries = 1)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.send_space"><code class="name flex">
<span>def <span class="ident">send_space</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Send spacebar command to <code>REBOUND</code> server at port to pause simulation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to send spacebar command to</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def send_space(self, port:int):
    &#34;&#34;&#34; Send spacebar command to `REBOUND` server at port to pause simulation.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to send spacebar command to
    &#34;&#34;&#34;
    self.check_port_feature()
    urllib3.request(method = &#34;GET&#34;,
                    url = f&#34;{self.server_path}:{port}/keyboard/32&#34;,
                    retries = False)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.simfunc_check"><code class="name flex">
<span>def <span class="ident">simfunc_check</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Get properties of <code>simfunc</code> and check if simfunc is in valid form.
Returns whether port is an argument.</p>
<h2 id="returns">Returns:</h2>
<p>simfunc_port : bool
Whether port is an argument in <code>simfunc</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simfunc_check(self)-&gt;bool:
    &#34;&#34;&#34; Get properties of `simfunc` and check if simfunc is in valid form.
        Returns whether port is an argument.

        Returns:
        --------
        simfunc_port : bool
            Whether port is an argument in `simfunc`
    &#34;&#34;&#34;
    if callable(self.simfunc) == False:
        raise TypeError(f&#34;simfunc must be a function. {type(self.simfunc)} was passed.&#34;)

    if inspect.signature(self.simfunc).parameters.get(&#34;port&#34;) is None:
        simfunc_port = False
    else:
        simfunc_port = True

    if (simfunc_port == True and 
        list(inspect.signature(self.simfunc).parameters).index(&#34;port&#34;) != 0):
        raise TypeError(f&#34;port must be the first argument in {self.simfunc.__name__}&#34;)

    return simfunc_port</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.start_all"><code class="name flex">
<span>def <span class="ident">start_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Unpause all simulations available on ports.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_all(self):
    &#34;&#34;&#34; Unpause all simulations available on ports.&#34;&#34;&#34;
    for port in self.current_open_ports():
        self.start_sim(port)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.start_sim"><code class="name flex">
<span>def <span class="ident">start_sim</span></span>(<span>self, port: int)</span>
</code></dt>
<dd>
<div class="desc"><p>Unpause simulation at port.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>port</code></strong> :&ensp;<code>int</code></dt>
<dd>Port of <code>REBOUND</code> server to unpause simulation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def start_sim(self, port:int):
    &#34;&#34;&#34; Unpause simulation at port.

        Parameters
        ----------
        port : int
            Port of `REBOUND` server to unpause simulation
    &#34;&#34;&#34;
    if REB_STATUS[self.fetch_sim(port)._status] == &#34;paused&#34;:
        self.send_space(port)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.validate_init"><code class="name flex">
<span>def <span class="ident">validate_init</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Validate ReboundParallel object after initialization.
Raises TypeError if any of the parameters are invalid.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_init(self):
    &#34;&#34;&#34; Validate ReboundParallel object after initialization.
        Raises TypeError if any of the parameters are invalid.
    &#34;&#34;&#34;
    if self.port0 &gt; 65535 or self.port0 &lt; 1024:
        raise TypeError(&#34;port0 must be a positive integer between 1024 and 65535&#34;)

    if type(self.cores) != int or self.cores &lt; 1:
        raise TypeError(&#34;cores must be a non-zero positive integer&#34;)

    if type(self.port_buffer) != int or self.port_buffer &lt; 1:
        raise TypeError(&#34;port_buffer must be a non-zero positive integer&#34;)

    if type(self.progressbar) != bool:
        raise TypeError(&#34;progressbar must be a boolean&#34;)</code></pre>
</details>
</dd>
<dt id="reboundp.parallel.ReboundParallel.verify_before_run"><code class="name flex">
<span>def <span class="ident">verify_before_run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Validate ReboundParallel object before parallel running.
Raises ValueError if any of the parameters are not set.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def verify_before_run(self):
    &#34;&#34;&#34; Validate ReboundParallel object before parallel running.
        Raises ValueError if any of the parameters are not set.
    &#34;&#34;&#34;
    if self.njobs is None:
        raise ValueError(&#34;njobs must be set before running&#34;)
    if self.ports_array is None:
        raise ValueError(&#34;ports_array must be set before running&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="reboundp.parallel.TimedBatchCompletionCallBack"><code class="flex name class">
<span>class <span class="ident">TimedBatchCompletionCallBack</span></span>
<span>(</span><span>dispatch_timestamp, batch_size, parallel)</span>
</code></dt>
<dd>
<div class="desc"><p>Custom callback for <code>joblib.parallel.Parallel</code> that prints progress to <code>stdout</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TimedBatchCompletionCallBack(joblib.parallel.BatchCompletionCallBack):
    &#34;&#34;&#34; Custom callback for `joblib.parallel.Parallel` that prints progress to `stdout`.&#34;&#34;&#34;
     
    def __call__(self, *args, **kwargs):
        if self.parallel.progressbar:
            print_progress(self.parallel.n_completed_tasks,
                           self.parallel.joblib_n_jobs, 
                           self.parallel.joblib_t0)

        return super().__call__(*args, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>joblib.parallel.BatchCompletionCallBack</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="reboundp" href="index.html">reboundp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="reboundp.parallel.print_progress" href="#reboundp.parallel.print_progress">print_progress</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="reboundp.parallel.ReboundParallel" href="#reboundp.parallel.ReboundParallel">ReboundParallel</a></code></h4>
<ul class="">
<li><code><a title="reboundp.parallel.ReboundParallel.check_port_feature" href="#reboundp.parallel.ReboundParallel.check_port_feature">check_port_feature</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.current_open_ports" href="#reboundp.parallel.ReboundParallel.current_open_ports">current_open_ports</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.end_all" href="#reboundp.parallel.ReboundParallel.end_all">end_all</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.end_all_current_sims" href="#reboundp.parallel.ReboundParallel.end_all_current_sims">end_all_current_sims</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.end_sim" href="#reboundp.parallel.ReboundParallel.end_sim">end_sim</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.fetch_sim" href="#reboundp.parallel.ReboundParallel.fetch_sim">fetch_sim</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.pause_all" href="#reboundp.parallel.ReboundParallel.pause_all">pause_all</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.pause_sim" href="#reboundp.parallel.ReboundParallel.pause_sim">pause_sim</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.process_jobs" href="#reboundp.parallel.ReboundParallel.process_jobs">process_jobs</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.reset_run" href="#reboundp.parallel.ReboundParallel.reset_run">reset_run</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.run" href="#reboundp.parallel.ReboundParallel.run">run</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.send_q" href="#reboundp.parallel.ReboundParallel.send_q">send_q</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.send_space" href="#reboundp.parallel.ReboundParallel.send_space">send_space</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.simfunc_check" href="#reboundp.parallel.ReboundParallel.simfunc_check">simfunc_check</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.start_all" href="#reboundp.parallel.ReboundParallel.start_all">start_all</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.start_sim" href="#reboundp.parallel.ReboundParallel.start_sim">start_sim</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.validate_init" href="#reboundp.parallel.ReboundParallel.validate_init">validate_init</a></code></li>
<li><code><a title="reboundp.parallel.ReboundParallel.verify_before_run" href="#reboundp.parallel.ReboundParallel.verify_before_run">verify_before_run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="reboundp.parallel.TimedBatchCompletionCallBack" href="#reboundp.parallel.TimedBatchCompletionCallBack">TimedBatchCompletionCallBack</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>